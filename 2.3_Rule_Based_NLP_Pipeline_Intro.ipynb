{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based NLP Pipeline\n",
    "\n",
    "If this title doesn't make any sense to you, don't worry for now. This notebook tends to explain the idea in plain language. You will know what it is and how it works after finishing this module. Let's start from a simple task with basic intuition.\n",
    "\n",
    "Suppose we have a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='The patient came in with fever.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we want to know if this sentence tells us about whether the patient has fever. What is your basic instinct to implement a solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naïve solution\n",
    "\n",
    "A very naïve solution could be to just find the string \"the patient came in with fever.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rule='The patient came in with fever'\n",
    "print(rule in input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this solution has a big problem. **What is it**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div title=\"Think about other cases. There may be a case like 'The pt got a fever.' or 'The pt developed a fever.' etc. What can you if you define rules like above.\">Hint</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better and neat solution\n",
    "\n",
    "As you may have noticed, there's a common characteristics in all the examples above. They all have the word \"fever.\" so probably we don't need to match all the boards in that sentence. Instead, we can simply check if a sentence has the keyword \"fever.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rule='fever'\n",
    "print(rule in input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "In fact, we just implemented a very simple named entity recognition solution! Of course it's not perfect, but it works. Named entity recognition is a subtask of NLP, which aims to identify the named entities or concepts in free text.\n",
    "\n",
    "In the example above,\"fever\" is our target concept. And we use simple string match to identify it from a sentence. Besides string match, another commonly used technique for named entity recognition is regular expression. \n",
    "\n",
    "Here is an exercise to try:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='He got a high temperature yesterday, T 102.5.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you identify the numeric temperature? Of course for the sentence of alone, you can still use string match. What if another case has a different number? In these cases, using regular expressions will be more efficient.\n",
    "\n",
    "If you don't know what regular expression is, you can check out [this notebook](2.1_Regular_Expression.ipynb) to taste its flavor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "## define your regular expression rule\n",
    "rule=r''\n",
    "pattern=re.compile(rule)\n",
    "res=pattern.search(input)\n",
    "## test your result, see if it matches your expectation\n",
    "res.group()=='T 102.5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced named entity recognition solutions can deal with syntactical similarities to match similar phrases or words, even in slightly different order; handle semantic similarities to match synonyms. We are not going to cover that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Usually, just to identify the named entities is not enough. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div title=\"What if it says 'Discussed about possible fever.' or 'No fever.'\">Hint</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Detection\n",
    "\n",
    "From the examples above, we now see that how important the context information is, especially for a clinical NLP, where many concepts are expressed as \"not exist.\" Now let's think about how to implement a context detector. Again, let's start from a very intuitive way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple solution\n",
    "Use regular expressions to match the context information and the target concepts together. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases =['The patient denies any fever.', 'The pt denies fever.',' The patient denies any chills or fever.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negated\n",
      "negated\n",
      "negated\n"
     ]
    }
   ],
   "source": [
    "rule=r'denies( \\w+)* fever'\n",
    "negation_pattern=re.compile(rule)\n",
    "for case in cases:\n",
    "    res=negation_pattern.search(case)\n",
    "    print('negated' if res is not None else 'not negated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this solution works. But, what if we have a long list of vocabularies for the target concepts? It seems we have to write every negation words combined with every synonym. That is not efficient and convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A smarter solution\n",
    "\n",
    "Let's separate the context identification with the target concept identification. Then, we can use code to check whether a target concept is mortified by any context information. This way we don't have to define the combinations by hand. And most importantly, once we build a set of context rules, we can reuse it in other similar tasks with or without minor tweaks.\n",
    "\n",
    "This is the first intuition of the context algorithm, which in fact the context algorithm does more than this. For more detailed information about the context algorithm, please check [this notebook](2.4_Intro_pyConText.ipynb).\n",
    "\n",
    "Here we are going to implement a very simple version of context algorithm:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negated\n"
     ]
    }
   ],
   "source": [
    "# First we need to define the rule to identify the target concept---fever\n",
    "target_rule=''\n",
    "\n",
    "# Second, we need to define the rules to identify the negation clue\n",
    "context_rule=''\n",
    "\n",
    "print('negated' if target_rule in input and context_rule in input else 'not negated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is a very simplified context algorithm. Your courage to study [this notebook](2.4_Intro_pyConText.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context detection within a document\n",
    "\n",
    "Now we've learned how to detect the context within a sentence. In real practice, we need to do the same thing to muliptle sentences within a document, which brings another issue. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='''....\n",
    "No vomiting, chest pain, shortness of breath,nausea, vomiting or chills.\n",
    "On operative day three, the patient developed a fever.\n",
    "....'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a human, we can certainly understand that the word \"No\" doesn't apply to the \"101.5 F\", because they are in different sentences. But how can we implement a solution to make a computer thinks in a similiar way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "This is why we need sentence segmentation. Sentence segmentation/detection is another subtask of NLP. It sets up the boundaries for the downstream NLP components to work on. Let's implement a single sentence segmenter to demonstrate how it helps for the context detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This segmenter a split sentence based on the new line character.\n",
    "sentences=input.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply our previous NER and context detection for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....:\t\tnegated\n",
      "No vomiting, chest pain, shortness of breath,nausea, vomiting or chills.:\t\tnegated\n",
      "On operative day three, the patient developed a fever.:\t\tnegated\n",
      "....:\t\tnegated\n"
     ]
    }
   ],
   "source": [
    "# Define the rule to identify the target concept---fever\n",
    "target_rule=''\n",
    "\n",
    "# Define the rules to identify the negation clue---no\n",
    "context_rule=''\n",
    "\n",
    "# Let's define a \"check\" function to mimic the context detection\n",
    "def check(sentence):\n",
    "    if target_rule in sentence:\n",
    "        if context_rule in sentence:\n",
    "            return \"negated\"\n",
    "        else:\n",
    "            return \"not negated\"\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence+':\\t\\t'+check(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, there are many ways to segment sentences. You can check [this notebook](2.3_Sentence_Segmentation.ipynb) to learn more about it.\n",
    "\n",
    "Now it seems we already have all the pieces that we need to build to the NLP pipeline.\n",
    "\n",
    "....No exactly. Remember our goal is to get a conclusion at document level to say whether a document has mentioned something that we care about. So, currently we only get conclusions at the sentence level, there is still one piece missing, where we need to make the document conclusion based on the sentence level conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification\n",
    "\n",
    "The rule-based the document classification is completely based on the rules that define inferences from the sentence level conclusions towards the document level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='''No nausea, vomiting, fever or chills.\n",
    "On operative day three, the patient spiked a temperature to 101.5 F.\n",
    "'''\n",
    "sentences=input.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it a simple, let's just use string match to find the target concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the rules in a list\n",
    "target_rules=['fever','101.5 F']\n",
    "\n",
    "# Define the rules to identify the negation clue---no\n",
    "context_rule='No'\n",
    "\n",
    "def check(sentence):\n",
    "    res='NA'\n",
    "    for target_rule in target_rules:        \n",
    "        if target_rule in sentence:        \n",
    "            if context_rule in sentence:\n",
    "                res=\"negated\"\n",
    "            else:\n",
    "                res=\"not negated\"\n",
    "    return res\n",
    "\n",
    "results=[]\n",
    "for sentence in sentences:\n",
    "    results.append(check(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of sentence Level conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negated', 'not negated', 'NA']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a simple function to make a conclusion  by taking this list as the input, and output three type of document level conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your document classifier\n",
    "def classifier(results):\n",
    "#   add your code\n",
    "    \n",
    "    \n",
    "    return \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Test your classifier\n",
    "print(classifier(results)==\"not negated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pipeline\n",
    "Now you may have some ideas about the concept: **pipeline**. We need several different NLP components, and oragnize them together in an appropriate order to process the input information.\n",
    "\n",
    "To sum up the key points we learned from above, a rule based NLP pipeline includes the following components:\n",
    "\n",
    "sentence segmenter, named entity recognizer, context detector, and a document classifier.\n",
    "\n",
    "Now, **forget what you have coded above, those are just used to help you understand** the functionality of each component and why we need it. \n",
    "\n",
    "Next, we are going to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a state-of-the-art rule-based NLP pipeline\n",
    "\n",
    "To build a state-of-art rule-based NLP pipeline, we definitely don't need to be start from scratch. Instead, we will just borrow the components from others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyRuSH.RuSH import RuSH\n",
    "from DocumentClassifier import FeatureInferencer\n",
    "from DocumentClassifier import DocumentInferencer\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from nlp_pneumonia_utils import markup_sentence\n",
    "from itemData import get_item_data\n",
    "from pyConTextNLP.utils import get_document_markups\n",
    "from visual import convertMarkups2DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because there are too many sentence segmentation rules,let's read them from an external file\n",
    "sentence_rules='KB/rush_rules.tsv'\n",
    "# you can point target_rules to a file path instead, if there are many rules\n",
    "target_rules='''\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: fever\n",
    "Regex: ''\n",
    "Type: FEVER\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: high temperature\n",
    "Regex: '1\\d\\d\\.\\d F'\n",
    "Type: FEVER'''\n",
    "# context rules are often lengthy, you can point context_rules to an external rule files instead\n",
    "context_rules='''Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'no'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "# define the feature inference rule\n",
    "feature_inference_rule='''\n",
    "#Conclusion type, Evidence type, Modifier values associated with the evidence\n",
    "NEGATED_CONCEPT,FEVER,DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "# define the document inference rule\n",
    "document_inference_rule='''\n",
    "#Conclusion Type at document level, Evidence type at mention level\n",
    "FEVER_DOC,FEVER\n",
    "\n",
    "#Default document type\n",
    "NO_FEVER\n",
    "'''\n",
    "\n",
    "sentence_segmenter = RuSH(sentence_rules)\n",
    "feature_inferencer=FeatureInferencer(feature_inference_rule)\n",
    "document_inferencer = DocumentInferencer(document_inference_rule)\n",
    "\n",
    "targets=get_item_data(target_rules)\n",
    "modifiers=get_item_data(context_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Let's split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sentence_segmenter.segToSentenceSpans(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence(0-37):\tNo nausea, vomiting, fever or chills.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence(38-106):\tOn operative day three, the patient spiked a temperature to 101.5 F.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the document was splitted into\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence({}-{}):\\t{}\".format(sentence.begin, sentence.end, input[sentence.begin:sentence.end]))\n",
    "    print('\\n'+'-'* 100+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Apply Context algorithm to each sentence\n",
    "\n",
    "The pyConText has already built in a NER and ConText detector, which can be applied in a single call. So we don't need to use them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________\n",
      "rawText: no nausea, vomiting, fever or chills.\n",
      "cleanedText: no nausea, vomiting, fever or chills.\n",
      "********************************\n",
      "TARGET: <id> 82260465953830595506096534863875374696 </id> <phrase> fever </phrase> <category> ['fever'] </category> \n",
      "----MODIFIED BY: <id> 82260466033058758020360872457419325032 </id> <phrase> no </phrase> <category> ['definite_negated_existence'] </category> \n",
      "__________________________________________\n",
      "\n",
      "__________________________________________\n",
      "rawText: on operative day three, the patient spiked a temperature to 101.5 f.\n",
      "cleanedText: on operative day three, the patient spiked a temperature to 101.5 f.\n",
      "********************************\n",
      "TARGET: <id> 82260466112286920534625210050963275368 </id> <phrase> 101.5 f </phrase> <category> ['fever'] </category> \n",
      "__________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initiate a pyConTextGraph to hold the pyConText output\n",
    "context_doc = pyConTextGraph.ConTextDocument()\n",
    "for sentence in sentences:\n",
    "    sentence_text=input[sentence.begin:sentence.end].lower()\n",
    "    m = markup_sentence(sentence_text, modifiers=modifiers, targets=targets)\n",
    "    context_doc.addMarkup(m)\n",
    "    context_doc.getSectionMarkups()\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<ConTextDocument>\n",
      "no nausea, vomiting, fever or chills. on operative day three, the patient spiked a temperature to 101.5 f. <section>\n",
      "<sectionLabel> document </sectionLabel>\n",
      "<sentence>\n",
      "<sentenceNumber> 0 </sentenceNumber>\n",
      "<sentenceOffset> 0 </sentenceOffset></sentence>\n",
      "\n",
      "<ConTextMarkup>\n",
      "<rawText> no nausea, vomiting, fever or chills. </rawText>\n",
      "<cleanText> no nausea, vomiting, fever or chills. </cleanText>\n",
      "<nodes>\n",
      "\n",
      "<node>\n",
      "<category> modifier </category>\n",
      "\n",
      "<tagObject>\n",
      "<id> 82260466033058758020360872457419325032 </id>\n",
      "<phrase> no </phrase>\n",
      "<literal> no </literal>\n",
      "<category> ['definite_negated_existence'] </category>\n",
      "<spanStart> 0 </spanStart>\n",
      "<spanStop> 2 </spanStop>\n",
      "<scopeStart> 2 </scopeStart>\n",
      "<scopeStop> 37 </scopeStop>\n",
      "</tagObject>\n",
      "<modifies>\n",
      "<modifiedNode> 82260465953830595506096534863875374696 </modifiedNode>\n",
      "</modifies>\n",
      "\n",
      "</node>\n",
      "\n",
      "<node>\n",
      "<category> target </category>\n",
      "\n",
      "<tagObject>\n",
      "<id> 82260465953830595506096534863875374696 </id>\n",
      "<phrase> fever </phrase>\n",
      "<literal> fever </literal>\n",
      "<category> ['fever'] </category>\n",
      "<spanStart> 21 </spanStart>\n",
      "<spanStop> 26 </spanStop>\n",
      "<scopeStart> 0 </scopeStart>\n",
      "<scopeStop> 37 </scopeStop>\n",
      "</tagObject>\n",
      "<modifiedBy>\n",
      "<modifyingNode> 82260466033058758020360872457419325032 </modifyingNode>\n",
      "<modifyingCategory> ['definite_negated_existence'] </modifyingCategory>\n",
      "</modifiedBy>\n",
      "\n",
      "</node>\n",
      "\n",
      "</nodes>\n",
      "<edges>\n",
      "\n",
      "<edge>\n",
      "<startNode> 82260466033058758020360872457419325032 </startNode>\n",
      "<endNode> 82260465953830595506096534863875374696 </endNode>\n",
      "\n",
      "</edge>\n",
      "\n",
      "</edges>\n",
      "</ConTextMarkup>\n",
      "<sentence>\n",
      "<sentenceNumber> 1 </sentenceNumber>\n",
      "<sentenceOffset> 38 </sentenceOffset></sentence>\n",
      "\n",
      "<ConTextMarkup>\n",
      "<rawText> on operative day three, the patient spiked a temperature to 101.5 f. </rawText>\n",
      "<cleanText> on operative day three, the patient spiked a temperature to 101.5 f. </cleanText>\n",
      "<nodes>\n",
      "\n",
      "<node>\n",
      "<category> target </category>\n",
      "\n",
      "<tagObject>\n",
      "<id> 82260466112286920534625210050963275368 </id>\n",
      "<phrase> 101.5 f </phrase>\n",
      "<literal> high temperature </literal>\n",
      "<category> ['fever'] </category>\n",
      "<spanStart> 60 </spanStart>\n",
      "<spanStop> 67 </spanStop>\n",
      "<scopeStart> 0 </scopeStart>\n",
      "<scopeStop> 68 </scopeStop>\n",
      "</tagObject>\n",
      "\n",
      "</node>\n",
      "\n",
      "</nodes>\n",
      "<edges>\n",
      "\n",
      "</edges>\n",
      "</ConTextMarkup>\n",
      "</section>\n",
      "\n",
      "</ConTextDocument>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the context output in XML format\n",
    "print(context_doc.getXML())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Apply document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphy output is not convenient to either use or display, let's convert it into [pandas](https://pandas.pydata.org/) [dataframes](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert graphic markups into dataframe    \n",
    "markups = get_document_markups(context_doc)\n",
    "annotations, relations, doc_txt = convertMarkups2DF(markups) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's easier to read the results in tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>markup_id</th>\n",
       "      <th>vis_category</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>txt</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T0</td>\n",
       "      <td>Target</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>fever</td>\n",
       "      <td>fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1</td>\n",
       "      <td>Modifier</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>definite_negated_existence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T2</td>\n",
       "      <td>Target</td>\n",
       "      <td>98</td>\n",
       "      <td>105</td>\n",
       "      <td>101.5 f</td>\n",
       "      <td>fever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  markup_id vis_category start  end      txt                        type\n",
       "0        T0       Target    21   26    fever                       fever\n",
       "1        T1     Modifier     0    2       no  definite_negated_existence\n",
       "2        T2       Target    98  105  101.5 f                       fever"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation_id</th>\n",
       "      <th>type</th>\n",
       "      <th>arg1_cate</th>\n",
       "      <th>arg1_id</th>\n",
       "      <th>arg2_cate</th>\n",
       "      <th>arg2_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R0</td>\n",
       "      <td>definite_negated_existence</td>\n",
       "      <td>Modifier</td>\n",
       "      <td>T1</td>\n",
       "      <td>Target</td>\n",
       "      <td>T0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relation_id                        type arg1_cate arg1_id arg2_cate arg2_id\n",
       "0          R0  definite_negated_existence  Modifier      T1    Target      T0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(annotations)\n",
    "display(relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the anntoations and relations as input to make the document inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After inferred from modifier values, we got these types:\n",
      " ['negated_concept', 'fever']\n",
      "\n",
      "Document concluesion: fever_doc\n"
     ]
    }
   ],
   "source": [
    "# apply inferences for document classication\n",
    "inferenced_types = feature_inferencer.process(annotations, relations)\n",
    "print('After inferred from modifier values, we got these types:\\n '+str(inferenced_types))\n",
    "doc_conclusion = document_inferencer.process(inferenced_types)\n",
    "print('\\nDocument concluesion: '+doc_conclusion )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import view_pycontext_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t      <iframe src = \"tmp/x.html\" frameborder=\"0\" width = \"850\" height = \"200\">\n",
       "\t\t\t         Sorry your browser does not support inline frames.\n",
       "\t\t\t      </iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_pycontext_output(context_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
